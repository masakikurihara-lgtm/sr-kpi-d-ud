import streamlit as st
import requests
import pandas as pd
from datetime import datetime
import calendar
from ftplib import FTP
import io
import logging
from bs4 import BeautifulSoup
import re

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)

# --- å®šæ•°è¨­å®š ---
# SHOWROOM ã‚ªãƒ¼ã‚¬ãƒŠã‚¤ã‚¶ãƒ¼ãƒšãƒ¼ã‚¸ã®ãƒ©ã‚¤ãƒ–KPI URL
SR_LIVE_KPI_URL = "https://www.showroom-live.com/organizer/live_kpi"

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def parse_cookie_string(cookie_string: str) -> dict:
    """
    ã‚»ãƒŸã‚³ãƒ­ãƒ³åŒºåˆ‡ã‚Šã®ã‚¯ãƒƒã‚­ãƒ¼æ–‡å­—åˆ—ã‚’requests.SessionãŒä½¿ç”¨ã§ãã‚‹è¾æ›¸å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚
    """
    cookies = {}
    if not cookie_string:
        return cookies
        
    for pair in cookie_string.split(';'):
        if '=' in pair:
            key, value = pair.split('=', 1)
            cookies[key.strip()] = value.strip()
            
    return cookies

def get_target_months():
    """
    2023å¹´9æœˆä»¥é™ã®æœˆã‚’ã€ç¾åœ¨ã®æœˆã¾ã§ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã—ã¾ã™ (ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆç”¨)ã€‚
    """
    months = []
    start_date = datetime(2023, 9, 1)
    now = datetime.now()
    
    current_date = start_date
    while current_date <= now.replace(day=1, hour=0, minute=0, second=0, microsecond=0):
        label = current_date.strftime("%Y/%m")
        months.append((label, current_date))
        
        if current_date.month == 12:
            current_date = datetime(current_date.year + 1, 1, 1)
        else:
            current_date = datetime(current_date.year, current_date.month + 1, 1)
            
    return months[::-1]

def get_month_start_end(dt: datetime):
    """
    æŒ‡å®šã•ã‚ŒãŸæœˆã®æœ€åˆã®æ—¥ã¨æœ€å¾Œã®æ—¥ã‚’ 'YYYY-MM-DD' å½¢å¼ã§è¿”ã—ã¾ã™ã€‚
    """
    year = dt.year
    month = dt.month
    
    start_date_str = f"{year}-{month:02d}-01"
    
    _, last_day = calendar.monthrange(year, month)
    end_date_str = f"{year}-{month:02d}-{last_day:02d}"
    
    return start_date_str, end_date_str

def parse_live_duration(duration_str: str) -> int:
    """
    (127m24s) ã®ã‚ˆã†ãªæ–‡å­—åˆ—ã‹ã‚‰é…ä¿¡æ™‚é–“(åˆ†)ã‚’æŠ½å‡ºã—ã€30ç§’ã§ç¹°ã‚Šä¸Šã’å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚
    """
    match = re.search(r'\((\d+)m(\d+)s\)', duration_str)
    if not match:
        return 0
    
    minutes = int(match.group(1))
    seconds = int(match.group(2))
    
    if seconds >= 30:
        return minutes + 1
    else:
        return minutes

def scrape_kpi_data(session: requests.Session, month_dt: datetime) -> pd.DataFrame:
    """
    æŒ‡å®šã•ã‚ŒãŸæœˆã®ãƒ©ã‚¤ãƒ–KPIãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€é…ä¿¡æ™‚é–“(åˆ†)ä»¥å¤–ã¯å…¨ã¦æ–‡å­—åˆ—ã¨ã—ã¦ä¿æŒã—ã¾ã™ã€‚
    ï¼ˆä¿®æ­£ç‚¹ï¼šã‚«ãƒ³ãƒã€ãƒã‚¤ãƒ•ãƒ³ã€ãƒ–ãƒ©ãƒ³ã‚¯ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã€æ•°å€¤ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åœæ­¢ï¼‰
    """
    month_label = month_dt.strftime("%Y/%m")
    start_date, end_date = get_month_start_end(month_dt)
    
    st.info(f"å‡¦ç†å¯¾è±¡æœˆ: **{month_label}** ({start_date} - {end_date})")
    
    all_records = []
    MAX_PAGES = 5 
    
    CSV_HEADERS = [
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ãƒ«ãƒ¼ãƒ ID", "é…ä¿¡æ—¥æ™‚", "é…ä¿¡æ™‚é–“(åˆ†)", "é€£ç¶šé…ä¿¡æ—¥æ•°", "ãƒ«ãƒ¼ãƒ å",
        "åˆè¨ˆè¦–è´æ•°", "è¦–è´ä¼šå“¡æ•°", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¼šå“¡æ•°", "SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡", "åˆãƒ«ãƒ¼ãƒ æ¥è¨ªè€…æ•°",
        "åˆSRæ¥è¨ªè€…æ•°", "çŸ­æ™‚é–“æ»åœ¨è€…æ•°", "ãƒ«ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—æ¸›æ•°",
        "Postäººæ•°", "ç²å¾—æ”¯æ´point", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "åˆã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "ã‚®ãƒ•ãƒˆæ•°",
        "ã‚®ãƒ•ãƒˆäººæ•°", "åˆã‚®ãƒ•ãƒˆäººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°æ•°", 
        "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°äººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGç·é¡", 
        "2023å¹´9æœˆä»¥å‰ã®ãŠã¾ã‘åˆ†(ç„¡å„ŸSG RSå¤–)"
    ]

    for page in range(1, MAX_PAGES + 1):
        url = f"{SR_LIVE_KPI_URL}?page={page}&room_id=&from_date={start_date}&to_date={end_date}"
        st.caption(f"-> ãƒšãƒ¼ã‚¸ {page} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­: {url}")
        
        try:
            response = session.get(url, timeout=30)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            st.error(f"HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ (ãƒšãƒ¼ã‚¸ {page}): {e}")
            break
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        table_body = soup.find('table', {'class': 'table-striped'}).find('tbody')
        if not table_body:
            st.info(f"ãƒšãƒ¼ã‚¸ {page}: é…ä¿¡ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
            
        rows = table_body.find_all('tr')
        
        data_found = False
        for row in rows:
            cols = row.find_all('td', {'class': 'delim'})
            if len(cols) != 27:
                continue
            
            data_found = True
            record = {}
            col_data = [c.get_text(separator=' ', strip=True) for c in cols]
            
            # 0. ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID, 1. ãƒ«ãƒ¼ãƒ ID
            record[CSV_HEADERS[0]] = col_data[0].strip()
            record[CSV_HEADERS[1]] = col_data[1].strip()
            
            # 2. é…ä¿¡æ—¥æ™‚ã€é…ä¿¡æ™‚é–“ï¼ˆåˆ†ãƒ»ç§’ï¼‰ã€‘ã®å‡¦ç†
            datetime_duration_str = col_data[2].strip() 
            
            # é…ä¿¡æ—¥æ™‚ (é–‹å§‹æ™‚åˆ») ã®æŠ½å‡ºã¨å½¢å¼å¤‰æ› (YYYY/MM/DD HH:MM:SS)
            datetime_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', datetime_duration_str)
            if datetime_match:
                start_datetime = datetime.strptime(datetime_match.group(1), '%Y-%m-%d %H:%M:%S')
                record[CSV_HEADERS[2]] = start_datetime.strftime('%Y/%m/%d %H:%M:%S')
            else:
                record[CSV_HEADERS[2]] = ""
            
            # é…ä¿¡æ™‚é–“(åˆ†) ã®æŠ½å‡ºã¨ç¹°ã‚Šä¸Šã’ (æ•°å€¤ã¨ã—ã¦ä¿æŒ)
            record[CSV_HEADERS[3]] = parse_live_duration(datetime_duration_str)
            
            # 4. é€£ç¶šé…ä¿¡æ—¥æ•° ã‹ã‚‰ 26. 2023å¹´9æœˆä»¥å‰ã®ãŠã¾ã‘åˆ†(ç„¡å„ŸSG RSå¤–) ã¾ã§ã®å‡¦ç†
            for i in range(3, len(col_data)):
                csv_col_index = i + 1
                value = col_data[i]
                
                # HTMLã‹ã‚‰å–å¾—ã—ãŸå€¤ã®å‘¨å›²ã®ç©ºç™½ã‚’å‰Šé™¤
                value = value.strip()
                
                # â˜…â˜…â˜… ä¿®æ­£æ¸ˆã¿: ã‚«ãƒ³ãƒã€ãƒã‚¤ãƒ•ãƒ³ã€ãƒ–ãƒ©ãƒ³ã‚¯ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä¸€åˆ‡ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã‚ãªã„ â˜…â˜…â˜…
                record[CSV_HEADERS[csv_col_index]] = value
            
            all_records.append(record)
            
        if not data_found:
             st.info(f"ãƒšãƒ¼ã‚¸ {page}: é…ä¿¡ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
             break
            
    if not all_records:
        st.warning(f"æœˆé–“ãƒ‡ãƒ¼ã‚¿ãŒå…¨ãå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {month_label}")
        return pd.DataFrame()

    df = pd.DataFrame(all_records)
    return df


def process_kpi_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®é‡è¤‡å‰Šé™¤ã¨ã€æœ€çµ‚çš„ãªæ•´å½¢ã®ã¿ã‚’è¡Œã„ã¾ã™ã€‚
    ï¼ˆä¿®æ­£ç‚¹ï¼šæ•°å€¤å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’å…¨ã¦å‰Šé™¤ã—ã€ã‚«ãƒ³ãƒã€ãƒã‚¤ãƒ•ãƒ³ã€ãƒ–ãƒ©ãƒ³ã‚¯ã‚’ãã®ã¾ã¾ç¶­æŒã—ã¾ã™ã€‚ï¼‰
    """
    if df.empty:
        return df
    
    # --- é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ ---
    # é…ä¿¡æ™‚é–“(åˆ†)ã¯æ•°å€¤ã ãŒã€é‡è¤‡åˆ¤å®šã«ã¯å•é¡Œãªã„ãŸã‚ãã®ã¾ã¾ä½¿ç”¨
    dedupe_cols = ["ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ãƒ«ãƒ¼ãƒ ID", "é…ä¿¡æ—¥æ™‚", "é…ä¿¡æ™‚é–“(åˆ†)"]
    initial_count = len(df)
    df.drop_duplicates(subset=dedupe_cols, keep='first', inplace=True)
    deduped_count = len(df)
    
    if initial_count > deduped_count:
        st.success(f"é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ {initial_count - deduped_count} ä»¶å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    
    # æœ€çµ‚çš„ãªCSVã®ä¸¦ã³é †ã«ã‚«ãƒ©ãƒ ã‚’æ•´ç†
    final_cols = [
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ãƒ«ãƒ¼ãƒ ID", "é…ä¿¡æ—¥æ™‚", "é…ä¿¡æ™‚é–“(åˆ†)", "é€£ç¶šé…ä¿¡æ—¥æ•°", "ãƒ«ãƒ¼ãƒ å",
        "åˆè¨ˆè¦–è´æ•°", "è¦–è´ä¼šå“¡æ•°", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¼šå“¡æ•°", "SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡", "åˆãƒ«ãƒ¼ãƒ æ¥è¨ªè€…æ•°",
        "åˆSRæ¥è¨ªè€…æ•°", "çŸ­æ™‚é–“æ»åœ¨è€…æ•°", "ãƒ«ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—æ¸›æ•°",
        "Postäººæ•°", "ç²å¾—æ”¯æ´point", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "åˆã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "ã‚®ãƒ•ãƒˆæ•°",
        "ã‚®ãƒ•ãƒˆäººæ•°", "åˆã‚®ãƒ•ãƒˆäººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°æ•°", 
        "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°äººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGç·é¡", 
        "2023å¹´9æœˆä»¥å‰ã®ãŠã¾ã‘åˆ†(ç„¡å„ŸSG RSå¤–)"
    ]
    
    df_final = df[final_cols].copy()
    
    # ã™ã¹ã¦ã®æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã«ã¤ã„ã¦ã€ä¸¡ç«¯ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’ç¢ºå®Ÿã«å‰Šé™¤
    for col in df_final.columns:
        if df_final[col].dtype == 'object':
            # ãƒ–ãƒ©ãƒ³ã‚¯ã€ãƒã‚¤ãƒ•ãƒ³ã€ã‚«ãƒ³ãƒã‚’ç¶­æŒã—ã¤ã¤ã€ä¸è¦ãªä¸¡ç«¯ã®ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿ã‚’æ’é™¤
            df_final[col] = df_final[col].astype(str).str.strip()
    
    # æ•°å€¤å‹ã§ã‚ã‚‹ã€Œé…ä¿¡æ™‚é–“(åˆ†)ã€ã‚‚ã€CSVå‡ºåŠ›æ™‚ã«ã‚«ãƒ³ãƒãŒå…¥ã‚‰ãªã„ã‚ˆã†intå‹ã«å¤‰æ›
    df_final['é…ä¿¡æ™‚é–“(åˆ†)'] = pd.to_numeric(df_final['é…ä¿¡æ™‚é–“(åˆ†)'], errors='coerce').fillna(0).astype(int)

    # df_finalã¯ã€æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚«ãƒ³ãƒãƒ»ãƒã‚¤ãƒ•ãƒ³å«ã‚€ï¼‰ã€æ•°å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆé…ä¿¡æ™‚é–“(åˆ†)ï¼‰ãŒæ··åœ¨ã—ãŸçŠ¶æ…‹ã§CSVå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚
    return df_final


def upload_to_ftp(df: pd.DataFrame, month_dt: datetime):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’CSVå½¢å¼ã«å¤‰æ›ã—ã€FTPã‚µãƒ¼ãƒãƒ¼ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    """
    if df.empty:
        st.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    # Secretsã‹ã‚‰FTPæ¥ç¶šæƒ…å ±ã‚’å–å¾—
    try:
        FTP_HOST = st.secrets["ftp"]["host"]
        FTP_USER = st.secrets["ftp"]["user"]
        FTP_PASS = st.secrets["ftp"]["password"]
        
        # â˜…â˜…â˜… ä¿®æ­£æ¸ˆã¿: Secretsã‹ã‚‰target_base_pathã‚’èª­ã¿è¾¼ã‚€ â˜…â˜…â˜…
        FTP_BASE_PATH_FROM_SECRETS = st.secrets["ftp"]["target_base_path"]
        
    except KeyError:
        st.error("âŒ Streamlit Secretsã‹ã‚‰FTPæ¥ç¶šæƒ…å ±ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    year_month = month_dt.strftime("%Y-%m")
    # ãƒ•ã‚¡ã‚¤ãƒ«å: YYYY-MM_all_all.csv
    filename = f"{year_month}_all_all.csv"
    
    # Secretsã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ‘ã‚¹ã‚’ãã®ã¾ã¾ä½¿ç”¨
    ftp_path = f"{FTP_BASE_PATH_FROM_SECRETS}{filename}"
    
    # CSVãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã§ä½œæˆ (UTF-8 with BOM)
    csv_buffer = io.StringIO()
    # æ•°å€¤ã‚«ãƒ©ãƒ ã¯æ•°å€¤ã¨ã—ã¦ã€æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ï¼ˆã‚«ãƒ³ãƒãƒ»ãƒã‚¤ãƒ•ãƒ³å«ã‚€ï¼‰ã¯æ–‡å­—åˆ—ã¨ã—ã¦to_csvã§æ›¸ãå‡ºã—
    df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
    csv_data = csv_buffer.getvalue()

    try:
        # FTPæ¥ç¶š
        with FTP(FTP_HOST) as ftp:
            ftp.encoding = 'utf-8'
            ftp.login(user=FTP_USER, passwd=FTP_PASS)
            
            ftp.storlines(f'STOR {ftp_path}', io.BytesIO(csv_data.encode('utf-8-sig')))
            
        st.success(f"âœ… FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: **{ftp_path}**")

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è©³ç´°ã«è¡¨ç¤º
        st.error(f"âŒ FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.warning(f"æ¥ç¶šæƒ…å ± (Host: {FTP_HOST}, User: {FTP_USER}) ãŒæ­£ã—ã„ã‹ã€ãŠã‚ˆã³ãƒ‘ã‚¹ **{ftp_path}** ã¸ã®æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


# --- Streamlitãƒ¡ã‚¤ãƒ³å‡¦ç† ---

def main():
    st.set_page_config(page_title="SHOWROOM ãƒ©ã‚¤ãƒ–KPIã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", layout="wide")
    #st.title("ãƒ©ã‚¤ãƒãƒ¼KPIãƒ‡ãƒ¼ã‚¿ è‡ªå‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ« (ãƒ©ã‚¤ãƒ–é…ä¿¡KPI)")
    st.markdown(
        "<h1 style='font-size:28px; text-align:left; color:#1f2937;'>SHOWROOM ãƒ©ã‚¤ãƒ–KPIã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</h1>",
        unsafe_allow_html=True
    )  
    st.markdown("---")

    # --- Secretsã‹ã‚‰æ©Ÿå¯†æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ ---
    try:
        AUTH_COOKIE_STRING = st.secrets["showroom"]["auth_cookie_string"]
        SESSION_COOKIE = parse_cookie_string(AUTH_COOKIE_STRING)
    except KeyError:
        st.error("âŒ Streamlit Secretsãƒ•ã‚¡ã‚¤ãƒ« (.streamlit/secrets.toml) ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€[showroom]ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®'auth_cookie_string'ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return
        
    # èªè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ
    session = requests.Session()
    session.cookies.update(SESSION_COOKIE)

    # 1. æœˆé¸æŠãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ã®ä½œæˆ
    month_options = get_target_months()
    month_labels = [label for label, _ in month_options]
    
    #st.header("1. å¯¾è±¡æœˆé¸æŠ")
    st.markdown("#### 1. å¯¾è±¡æœˆé¸æŠ")
    
    # è¤‡æ•°æœˆé¸æŠ (ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆ)
    selected_labels = st.multiselect(
        "å‡¦ç†å¯¾è±¡ã®é…ä¿¡æœˆã‚’é¸æŠã—ã¦ãã ã•ã„ (è¤‡æ•°é¸æŠå¯èƒ½):",
        options=month_labels,
        default=month_labels[:1]
    )

    if not selected_labels:
        st.warning("å‡¦ç†å¯¾è±¡ã®æœˆã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        return
        
    selected_months = [
        dt for label, dt in month_options if label in selected_labels
    ]
    
    st.info(f"é¸æŠã•ã‚ŒãŸæœˆ: **{', '.join(selected_labels)}**")
    
    #st.header("2. ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ")
    st.markdown("#### 2. ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ")
    
    # 3. å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("ğŸš€ KPIãƒ‡ãƒ¼ã‚¿ã®å…¨ã¦ã‚’å–å¾—ãƒ»FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ", type="primary"):
        all_success = True
        with st.spinner("å‡¦ç†ä¸­: é¸æŠã•ã‚ŒãŸæœˆã®KPIãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»æ•´å½¢ã—ã¦ã„ã¾ã™..."):
            
            for month_dt in selected_months:
                #st.subheader(f"ğŸ“… {month_dt.strftime('%Y/%m')} ã®å‡¦ç†ã‚’é–‹å§‹")
                st.markdown(f"##### ğŸ“… {month_dt.strftime('%Y/%m')} ã®å‡¦ç†ã‚’é–‹å§‹")
                
                # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—
                raw_df = scrape_kpi_data(session, month_dt)
                
                if raw_df.empty:
                    st.warning(f"âš ï¸ {month_dt.strftime('%Y/%m')} ã®ãƒ‡ãƒ¼ã‚¿ã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    all_success = False
                    st.markdown("---")
                    continue
                
                # 2. ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã¨é‡è¤‡å‰Šé™¤
                processed_df = process_kpi_data(raw_df)
                
                if not processed_df.empty:
                    # â˜…â˜…â˜… ä¿®æ­£æ¸ˆã¿: Streamlitã®TypeErrorå›é¿ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ â˜…â˜…â˜…
                    # st.dataframe(processed_df.head(), caption=f"{month_dt.strftime('%Y/%m')} ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å…¨ {len(processed_df)} ä»¶)", use_container_width=True)
                    st.success(f"ãƒ‡ãƒ¼ã‚¿ ({len(processed_df)} ä»¶) ã‚’æ­£å¸¸ã«å–å¾—ãƒ»æ•´å½¢ã—ã¾ã—ãŸã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

                    # 3. FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                    upload_to_ftp(processed_df, month_dt)
                else:
                    st.warning(f"âš ï¸ {month_dt.strftime('%Y/%m')} ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€æ•´å½¢ï¼ˆé‡è¤‡å‰Šé™¤ãªã©ï¼‰å¾Œã«æ®‹ã£ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ0ä»¶ã§ã—ãŸã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    all_success = False
                
                st.markdown("---")

        st.balloons()
        if all_success:
            st.success("ğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            st.info("å‡¦ç†ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ä¸€éƒ¨ã®æœˆã§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‹ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
if __name__ == "__main__":
    main()