import streamlit as st
import requests
import pandas as pd
from datetime import datetime
import calendar
from ftplib import FTP
import io
import logging
from bs4 import BeautifulSoup
import re

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)

# --- å®šæ•°è¨­å®š ---
# SHOWROOM ã‚ªãƒ¼ã‚¬ãƒŠã‚¤ã‚¶ãƒ¼ãƒšãƒ¼ã‚¸ã®ãƒ©ã‚¤ãƒ–KPI URL
SR_LIVE_KPI_URL = "https://www.showroom-live.com/organizer/live_kpi"

# KPIãƒ‡ãƒ¼ã‚¿ã®æ ¼ç´å…ˆãƒ‘ã‚¹ (è¦ä»¶: https://mksoul-pro.com/showroom/csv/YYYY-MM_all_all.csv ã«åŸºã¥ã)
# Secretsã®target_base_pathã¯ç„¡è¦–ã—ã€ã“ã¡ã‚‰ã‚’å›ºå®šã§ä½¿ç”¨ã—ã¾ã™ã€‚
FTP_BASE_PATH = "/showroom/csv/"


# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def parse_cookie_string(cookie_string: str) -> dict:
    """
    ã‚»ãƒŸã‚³ãƒ­ãƒ³åŒºåˆ‡ã‚Šã®ã‚¯ãƒƒã‚­ãƒ¼æ–‡å­—åˆ—ã‚’requests.SessionãŒä½¿ç”¨ã§ãã‚‹è¾æ›¸å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚
    """
    cookies = {}
    if not cookie_string:
        return cookies
        
    for pair in cookie_string.split(';'):
        # ãƒšã‚¢ã«'='ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã€æœ€åˆã®'='ã§åˆ†å‰²
        if '=' in pair:
            key, value = pair.split('=', 1)
            cookies[key.strip()] = value.strip()
            
    return cookies

def get_target_months():
    """
    2023å¹´9æœˆä»¥é™ã®æœˆã‚’ã€ç¾åœ¨ã®æœˆã¾ã§ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã—ã¾ã™ (ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆç”¨)ã€‚
    """
    months = []
    # è¦ä»¶: 2023å¹´9æœˆä»¥é™
    start_date = datetime(2023, 9, 1)
    # ç¾åœ¨ã®æ—¥ä»˜
    now = datetime.now()
    
    current_date = start_date
    while current_date <= now.replace(day=1, hour=0, minute=0, second=0, microsecond=0):
        # YYYY/MM å½¢å¼ã®ãƒ©ãƒ™ãƒ«ã¨ datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¿ãƒ—ãƒ«ã§ä¿å­˜
        label = current_date.strftime("%Y/%m")
        months.append((label, current_date))
        
        # æ¬¡ã®æœˆã«é€²ã‚€
        if current_date.month == 12:
            current_date = datetime(current_date.year + 1, 1, 1)
        else:
            current_date = datetime(current_date.year, current_date.month + 1, 1)
            
    # æ–°ã—ã„æœˆãŒä¸Šã«æ¥ã‚‹ã‚ˆã†ã«é€†é †ã«ã™ã‚‹
    return months[::-1]

def get_month_start_end(dt: datetime):
    """
    æŒ‡å®šã•ã‚ŒãŸæœˆã®æœ€åˆã®æ—¥ã¨æœ€å¾Œã®æ—¥ã‚’ 'YYYY-MM-DD' å½¢å¼ã§è¿”ã—ã¾ã™ã€‚
    """
    year = dt.year
    month = dt.month
    
    # é–‹å§‹æ—¥ (YYYY-MM-01)
    start_date_str = f"{year}-{month:02d}-01"
    
    # çµ‚äº†æ—¥ (ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã«å¿ å®Ÿã«ã€æœˆæœ«ã®æ—¥ä»˜ã‚’å–å¾—)
    _, last_day = calendar.monthrange(year, month)
    end_date_str = f"{year}-{month:02d}-{last_day:02d}"
    
    return start_date_str, end_date_str

def parse_live_duration(duration_str: str) -> int:
    """
    (127m24s) ã®ã‚ˆã†ãªæ–‡å­—åˆ—ã‹ã‚‰é…ä¿¡æ™‚é–“(åˆ†)ã‚’æŠ½å‡ºã—ã€30ç§’ã§ç¹°ã‚Šä¸Šã’å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚
    """
    match = re.search(r'\((\d+)m(\d+)s\)', duration_str)
    if not match:
        return 0
    
    minutes = int(match.group(1))
    seconds = int(match.group(2))
    
    # 30ç§’ä»¥é™ã§ã‚ã‚Œã°ç¹°ã‚Šä¸Šã’
    if seconds >= 30:
        return minutes + 1
    else:
        return minutes

def scrape_kpi_data(session: requests.Session, month_dt: datetime) -> pd.DataFrame:
    """
    æŒ‡å®šã•ã‚ŒãŸæœˆã®ãƒ©ã‚¤ãƒ–KPIãƒ‡ãƒ¼ã‚¿ã‚’æœ€å¤§5ãƒšãƒ¼ã‚¸ã¾ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚
    """
    month_label = month_dt.strftime("%Y/%m")
    start_date, end_date = get_month_start_end(month_dt)
    
    st.info(f"å‡¦ç†å¯¾è±¡æœˆ: **{month_label}** ({start_date} - {end_date})")
    
    all_records = []
    MAX_PAGES = 5 # è¦ä»¶ã«ã‚ˆã‚Š5ãƒšãƒ¼ã‚¸
    
    # CSVæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ãŸã€æœ€çµ‚çš„ãª28åˆ—ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å®šç¾©
    CSV_HEADERS = [
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ãƒ«ãƒ¼ãƒ ID", "é…ä¿¡æ—¥æ™‚", "é…ä¿¡æ™‚é–“(åˆ†)", "é€£ç¶šé…ä¿¡æ—¥æ•°", "ãƒ«ãƒ¼ãƒ å",
        "åˆè¨ˆè¦–è´æ•°", "è¦–è´ä¼šå“¡æ•°", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¼šå“¡æ•°", "SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡", "åˆãƒ«ãƒ¼ãƒ æ¥è¨ªè€…æ•°",
        "åˆSRæ¥è¨ªè€…æ•°", "çŸ­æ™‚é–“æ»åœ¨è€…æ•°", "ãƒ«ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—æ¸›æ•°",
        "Postäººæ•°", "ç²å¾—æ”¯æ´point", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "åˆã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "ã‚®ãƒ•ãƒˆæ•°",
        "ã‚®ãƒ•ãƒˆäººæ•°", "åˆã‚®ãƒ•ãƒˆäººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°æ•°", 
        "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°äººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGç·é¡", 
        "2023å¹´9æœˆä»¥å‰ã®ãŠã¾ã‘åˆ†(ç„¡å„ŸSG RSå¤–)"
    ]

    for page in range(1, MAX_PAGES + 1):
        url = f"{SR_LIVE_KPI_URL}?page={page}&room_id=&from_date={start_date}&to_date={end_date}"
        st.caption(f"-> ãƒšãƒ¼ã‚¸ {page} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­: {url}")
        
        try:
            response = session.get(url, timeout=30)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            st.error(f"HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ (ãƒšãƒ¼ã‚¸ {page}): {e}")
            break
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®tbodyã‚’æ¢ã™
        table_body = soup.find('table', {'class': 'table-striped'}).find('tbody')
        if not table_body:
            st.info(f"ãƒšãƒ¼ã‚¸ {page}: ãƒ†ãƒ¼ãƒ–ãƒ«ãƒœãƒ‡ã‚£ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é…ä¿¡ãƒ‡ãƒ¼ã‚¿ãªã—ã¨ã¿ãªã—ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
            
        rows = table_body.find_all('tr')
        
        # é…ä¿¡ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å‡¦ç†
        data_found = False
        for row in rows:
            cols = row.find_all('td', {'class': 'delim'})
            if len(cols) != 27: # HTMLã®åˆ—æ•°ã¯27 (é…ä¿¡æ—¥æ™‚/æ™‚é–“å«ã‚€)
                continue
            
            data_found = True
            record = {}
            col_data = [c.get_text(separator=' ', strip=True) for c in cols]
            
            # 0. ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID, 1. ãƒ«ãƒ¼ãƒ ID
            record[CSV_HEADERS[0]] = col_data[0].strip()
            record[CSV_HEADERS[1]] = col_data[1].strip()
            
            # 2. é…ä¿¡æ—¥æ™‚ã€é…ä¿¡æ™‚é–“ï¼ˆåˆ†ãƒ»ç§’ï¼‰ã€‘ã®å‡¦ç†
            datetime_duration_str = col_data[2].strip() # ä¾‹: '2025-10-31 21:06:59 - 23:14:23 (127m24s)'
            
            # é…ä¿¡æ—¥æ™‚ (é–‹å§‹æ™‚åˆ») ã®æŠ½å‡º
            datetime_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', datetime_duration_str)
            if datetime_match:
                # æœ€çµ‚çš„ãªCSVã®æ›¸å¼ã«åˆã‚ã›ã‚‹ãŸã‚ YYYY/MM/DD HH:MM:SS ã«å¤‰æ›
                start_datetime = datetime.strptime(datetime_match.group(1), '%Y-%m-%d %H:%M:%S')
                record[CSV_HEADERS[2]] = start_datetime.strftime('%Y/%m/%d %H:%M:%S')
            else:
                record[CSV_HEADERS[2]] = ""
            
            # é…ä¿¡æ™‚é–“(åˆ†) ã®æŠ½å‡ºã¨ç¹°ã‚Šä¸Šã’
            record[CSV_HEADERS[3]] = parse_live_duration(datetime_duration_str)
            
            # 4. é€£ç¶šé…ä¿¡æ—¥æ•° ã‹ã‚‰ 26. 2023å¹´9æœˆä»¥å‰ã®ãŠã¾ã‘åˆ†(ç„¡å„ŸSG RSå¤–) ã¾ã§ã®å‡¦ç†
            for i in range(3, len(col_data)):
                html_col_index = i
                csv_col_index = i + 1
                
                value = col_data[html_col_index]
                
                # æ•°å€¤ãƒ»ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ â˜…â˜…â˜…: csv_col_index >= 6 (åˆè¨ˆè¦–è´æ•°) ã‹ã‚‰ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹
                if csv_col_index >= 6 and csv_col_index <= 27: 
                    # ã‚«ãƒ³ãƒ(,)é™¤å»ã€ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ(%)é™¤å»ã€ãƒã‚¤ãƒ•ãƒ³(-)ã‚’ç©ºæ–‡å­—åˆ—ã«
                    value = value.replace(',', '').replace('-', '').replace('%', '')
                
                record[CSV_HEADERS[csv_col_index]] = value.strip()
            
            all_records.append(record)
            
        if not data_found:
             st.info(f"ãƒšãƒ¼ã‚¸ {page}: é…ä¿¡ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
             break
            
    if not all_records:
        st.warning(f"æœˆé–“ãƒ‡ãƒ¼ã‚¿ãŒå…¨ãå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {month_label}")
        return pd.DataFrame()

    df = pd.DataFrame(all_records)
    return df


# streamlit_kpi_app.py å†…ã® process_kpi_data é–¢æ•°

def process_kpi_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•´å½¢ã€é‡è¤‡å‰Šé™¤ã€ãƒ‡ãƒ¼ã‚¿å‹ã®å¼·åˆ¶å¤‰æ›ã‚’è¡Œã„ã¾ã™ã€‚
    """
    if df.empty:
        return df

    # --- ãƒ‡ãƒ¼ã‚¿å‹ã®èª¿æ•´ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° ---
    numeric_cols = [
        "é…ä¿¡æ™‚é–“(åˆ†)", "é€£ç¶šé…ä¿¡æ—¥æ•°", "åˆè¨ˆè¦–è´æ•°", "è¦–è´ä¼šå“¡æ•°", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¼šå“¡æ•°", 
        "åˆãƒ«ãƒ¼ãƒ æ¥è¨ªè€…æ•°", "åˆSRæ¥è¨ªè€…æ•°", "çŸ­æ™‚é–“æ»åœ¨è€…æ•°", "ãƒ«ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", 
        "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—æ¸›æ•°", "Postäººæ•°", "ç²å¾—æ”¯æ´point", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", 
        "åˆã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "ã‚®ãƒ•ãƒˆæ•°", "ã‚®ãƒ•ãƒˆäººæ•°", "åˆã‚®ãƒ•ãƒˆäººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°æ•°", 
        "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°äººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGç·é¡", 
        "2023å¹´9æœˆä»¥å‰ã®ãŠã¾ã‘åˆ†(ç„¡å„ŸSG RSå¤–)"
    ]
    
    # SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡ (%) ã®ã¿å€‹åˆ¥ã«å‡¦ç† (floatã¨ã—ã¦æ‰±ã†)
    # å€¤ãŒãªã„å ´åˆã¯'0'ã¨ã—ã¦å‡¦ç†
    df['SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡'] = pd.to_numeric(
        df['SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡'].astype(str).str.replace(r'[^\d.]', '', regex=True).replace('', '0'), 
        errors='coerce'
    ).fillna(0).round(1)
    
    # æ•´æ•°ã‚«ãƒ©ãƒ ã®å‡¦ç†ã‚’å¼·åŒ– (éæ•°å­—ã‚’å¾¹åº•é™¤å»ã—ã€intã«å¤‰æ›)
    for col in numeric_cols:
        # æ–‡å­—åˆ—ã«å¤‰æ›å¾Œã€æ•°å­—ã€ãƒã‚¤ãƒ•ãƒ³(-), å°æ•°ç‚¹(.)ä»¥å¤–ã®æ–‡å­—ã‚’ã™ã¹ã¦å‰Šé™¤
        # ãŸã ã—ã€ã“ã“ã§ã¯æ•´æ•°ã¨ã—ã¦æ‰±ã†ã‚«ãƒ©ãƒ ãŒã»ã¨ã‚“ã©ãªã®ã§ã€å°æ•°ç‚¹ã‚‚å‰Šé™¤ã—ã¾ã™ã€‚
        cleaned_series = df[col].astype(str).str.replace(r'[^\d-]', '', regex=True)
        
        # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã€ç©ºæ–‡å­—åˆ—ã«ãªã£ãŸå ´åˆã¯'0'ã«ç½®æ›
        cleaned_series = cleaned_series.replace('', '0')

        # to_numericã§æ•°å€¤ã«å¤‰æ›ã—ã€NaNã‚’0ã§åŸ‹ã‚ã¦intã«å¤‰æ›
        df[col] = pd.to_numeric(cleaned_series, errors='coerce').fillna(0).astype(int)

    # --- é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ ---
    # ã‚­ãƒ¼: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆIDã€ãƒ«ãƒ¼ãƒ IDã€é…ä¿¡æ—¥æ™‚ã€é…ä¿¡æ™‚é–“(åˆ†)
    dedupe_cols = ["ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ãƒ«ãƒ¼ãƒ ID", "é…ä¿¡æ—¥æ™‚", "é…ä¿¡æ™‚é–“(åˆ†)"]
    
    initial_count = len(df)
    df.drop_duplicates(subset=dedupe_cols, keep='first', inplace=True)
    deduped_count = len(df)
    
    if initial_count > deduped_count:
        st.success(f"é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ {initial_count - deduped_count} ä»¶å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    
    # æœ€çµ‚çš„ãªCSVã®ä¸¦ã³é †ã«ã‚«ãƒ©ãƒ ã‚’æ•´ç†
    final_cols = [
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ãƒ«ãƒ¼ãƒ ID", "é…ä¿¡æ—¥æ™‚", "é…ä¿¡æ™‚é–“(åˆ†)", "é€£ç¶šé…ä¿¡æ—¥æ•°", "ãƒ«ãƒ¼ãƒ å",
        "åˆè¨ˆè¦–è´æ•°", "è¦–è´ä¼šå“¡æ•°", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¼šå“¡æ•°", "SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡", "åˆãƒ«ãƒ¼ãƒ æ¥è¨ªè€…æ•°",
        "åˆSRæ¥è¨ªè€…æ•°", "çŸ­æ™‚é–“æ»åœ¨è€…æ•°", "ãƒ«ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—æ¸›æ•°",
        "Postäººæ•°", "ç²å¾—æ”¯æ´point", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "åˆã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "ã‚®ãƒ•ãƒˆæ•°",
        "ã‚®ãƒ•ãƒˆäººæ•°", "åˆã‚®ãƒ•ãƒˆäººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°æ•°", 
        "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°äººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGç·é¡", 
        "2023å¹´9æœˆä»¥å‰ã®ãŠã¾ã‘åˆ†(ç„¡å„ŸSG RSå¤–)"
    ]
    
    # æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™å‰ã«ã€Streamlitã¨ã®äº’æ›æ€§ã®ãŸã‚ä¸€æ—¦ã‚³ãƒ”ãƒ¼ã‚’è¿”ã—ã¾ã™
    return df[final_cols].copy()


def upload_to_ftp(df: pd.DataFrame, month_dt: datetime):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’CSVå½¢å¼ã«å¤‰æ›ã—ã€FTPã‚µãƒ¼ãƒãƒ¼ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    """
    if df.empty:
        st.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    # Secretsã‹ã‚‰FTPæ¥ç¶šæƒ…å ±ã‚’å–å¾—
    try:
        FTP_HOST = st.secrets["ftp"]["host"]
        FTP_USER = st.secrets["ftp"]["user"]
        FTP_PASS = st.secrets["ftp"]["password"]
    except KeyError:
        st.error("âŒ Streamlit Secretsã‹ã‚‰FTPæ¥ç¶šæƒ…å ±ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    year_month = month_dt.strftime("%Y-%m")
    # ãƒ•ã‚¡ã‚¤ãƒ«å: YYYY-MM_all_all.csv
    filename = f"{year_month}_all_all.csv"
    # FTP BASE PATHã¯ã‚³ãƒ¼ãƒ‰ã§å®šç¾©ã—ãŸ/showroom/csv/ã‚’ä½¿ç”¨
    ftp_path = f"{FTP_BASE_PATH}{filename}"
    
    # CSVãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã§ä½œæˆ (UTF-8 with BOM)
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
    csv_data = csv_buffer.getvalue()

    try:
        # FTPæ¥ç¶š
        with FTP(FTP_HOST) as ftp:
            ftp.encoding = 'utf-8'
            ftp.login(user=FTP_USER, passwd=FTP_PASS)
            
            # storlinesã‚’ä½¿ç”¨ã—ã¦ã€æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã®å•é¡Œã‚’å›é¿ã—ã¤ã¤ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            ftp.storlines(f'STOR {ftp_path}', io.BytesIO(csv_data.encode('utf-8-sig')))
            
        st.success(f"âœ… FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: **{ftp_path}**")

    except Exception as e:
        st.error(f"âŒ FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.warning(f"æ¥ç¶šæƒ…å ± (Host: {FTP_HOST}, User: {FTP_USER}) ãŒæ­£ã—ã„ã‹ã€ãŠã‚ˆã³ãƒ‘ã‚¹ **{ftp_path}** ã¸ã®æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


# streamlit_kpi_app.py å†…ã® main é–¢æ•°

# --- Streamlitãƒ¡ã‚¤ãƒ³å‡¦ç† ---

def main():
    st.set_page_config(page_title="SHOWROOM KPIãƒ‡ãƒ¼ã‚¿ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ«", layout="wide")
    st.title("ãƒ©ã‚¤ãƒãƒ¼KPIãƒ‡ãƒ¼ã‚¿ è‡ªå‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ« (ãƒ©ã‚¤ãƒ–é…ä¿¡KPI)")
    st.markdown("---")

    # --- Secretsã‹ã‚‰æ©Ÿå¯†æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ ---
    try:
        AUTH_COOKIE_STRING = st.secrets["showroom"]["auth_cookie_string"]
        SESSION_COOKIE = parse_cookie_string(AUTH_COOKIE_STRING)
    except KeyError:
        st.error("âŒ Streamlit Secretsãƒ•ã‚¡ã‚¤ãƒ« (.streamlit/secrets.toml) ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€[showroom]ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®'auth_cookie_string'ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return
        
    # èªè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ
    session = requests.Session()
    session.cookies.update(SESSION_COOKIE)

    # 1. æœˆé¸æŠãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ã®ä½œæˆ
    month_options = get_target_months()
    month_labels = [label for label, _ in month_options]
    
    st.header("1. å¯¾è±¡æœˆé¸æŠ")
    
    # è¤‡æ•°æœˆé¸æŠ (ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆ)
    selected_labels = st.multiselect(
        "å‡¦ç†å¯¾è±¡ã®é…ä¿¡æœˆã‚’é¸æŠã—ã¦ãã ã•ã„ (è¤‡æ•°é¸æŠå¯èƒ½):",
        options=month_labels,
        default=month_labels[:1]
    )

    if not selected_labels:
        st.warning("å‡¦ç†å¯¾è±¡ã®æœˆã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        return
        
    # é¸æŠã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‹ã‚‰datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æŠ½å‡º
    selected_months = [
        dt for label, dt in month_options if label in selected_labels
    ]
    
    st.info(f"é¸æŠã•ã‚ŒãŸæœˆ: **{', '.join(selected_labels)}**")
    
    st.header("2. ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ")
    
    # 3. å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("ğŸš€ KPIãƒ‡ãƒ¼ã‚¿ã®å…¨ã¦ã‚’å–å¾—ãƒ»FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ", type="primary"):
        all_success = True
        with st.spinner("å‡¦ç†ä¸­: é¸æŠã•ã‚ŒãŸæœˆã®KPIãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»æ•´å½¢ã—ã¦ã„ã¾ã™..."):
            
            # é¸æŠã•ã‚ŒãŸæœˆã‚’é †ç•ªã«å‡¦ç†
            for month_dt in selected_months:
                st.subheader(f"ğŸ“… {month_dt.strftime('%Y/%m')} ã®å‡¦ç†ã‚’é–‹å§‹")
                
                # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—
                raw_df = scrape_kpi_data(session, month_dt)
                
                # raw_dfãŒç©ºãªã‚‰ã™ãã«æ¬¡ã®æœˆã«ã‚¹ã‚­ãƒƒãƒ—
                if raw_df.empty:
                    st.warning(f"âš ï¸ {month_dt.strftime('%Y/%m')} ã®ãƒ‡ãƒ¼ã‚¿ã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    all_success = False
                    st.markdown("---")
                    continue
                
                # 2. ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã¨é‡è¤‡å‰Šé™¤
                processed_df = process_kpi_data(raw_df)
                
                # æ•´å½¢å¾Œã‚‚ç©ºã§ãªã„ã‹ç¢ºèªï¼ˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼‰
                if not processed_df.empty:
                    # â˜…â˜…â˜… æœ€çµ‚ä¿®æ­£ç®‡æ‰€ï¼šst.dataframe()ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ â˜…â˜…â˜…
                    # st.dataframe(processed_df.head(), caption=f"{month_dt.strftime('%Y/%m')} ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å…¨ {len(processed_df)} ä»¶)", use_container_width=True)
                    st.success(f"ãƒ‡ãƒ¼ã‚¿ ({len(processed_df)} ä»¶) ã‚’æ­£å¸¸ã«å–å¾—ãƒ»æ•´å½¢ã—ã¾ã—ãŸã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

                    # 3. FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                    upload_to_ftp(processed_df, month_dt)
                else:
                    st.warning(f"âš ï¸ {month_dt.strftime('%Y/%m')} ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€æ•´å½¢ï¼ˆé‡è¤‡å‰Šé™¤ãªã©ï¼‰å¾Œã«æ®‹ã£ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ0ä»¶ã§ã—ãŸã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    all_success = False
                
                st.markdown("---") # æœˆã®åŒºåˆ‡ã‚Šç·š

        st.balloons()
        if all_success:
            st.success("ğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            st.info("å‡¦ç†ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ä¸€éƒ¨ã®æœˆã§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‹ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
if __name__ == "__main__":
    main()